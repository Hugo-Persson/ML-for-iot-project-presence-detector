{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "b1d4af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras as K, nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "e3e06b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded group_room_2_9_dec_presence.wav label=1.0 samples=1920000 sr=16000\n",
      "Loaded group_room_3_yazan_lab_9_dec_presence.wav label=1.0 samples=960000 sr=16000\n",
      "Loaded group_room_4_9_dec_no_presence.wav label=0.0 samples=13312 sr=16000\n",
      "Loaded group_room_4_9_dec_presence.wav label=1.0 samples=1920256 sr=16000\n",
      "Loaded group_room_5_9_dec_presence.wav label=1.0 samples=1920000 sr=16000\n",
      "Loaded group_room_9_dec_presence.wav label=1.0 samples=960000 sr=16000\n",
      "Loaded test_presence.wav label=1.0 samples=80128 sr=16000\n",
      "Loaded training-lax_no_presence.wav label=0.0 samples=960000 sr=16000\n",
      "Loaded training-lax_presence.wav label=1.0 samples=960256 sr=16000\n",
      "Loaded training2-lax_no_presence.wav label=0.0 samples=960000 sr=16000\n",
      "Loaded training2-lax_presence.wav label=1.0 samples=960000 sr=16000\n"
     ]
    }
   ],
   "source": [
    "# Discover labeled wav files and load audio\n",
    "wav_files = sorted(Path('.').glob('*.wav'))\n",
    "labeled_files = []\n",
    "for wav_path in wav_files:\n",
    "    stem = wav_path.stem\n",
    "    if 'no_presence' in stem:\n",
    "        label = 0.0\n",
    "    elif 'presence' in stem:\n",
    "        label = 1.0\n",
    "    else:\n",
    "        continue\n",
    "    labeled_files.append((stem, wav_path, label))\n",
    "\n",
    "if not labeled_files:\n",
    "    raise ValueError(\"No labeled wav files found (filenames should contain 'presence' or 'no_presence').\")\n",
    "\n",
    "dataset = []\n",
    "sample_rate = None\n",
    "for stem, wav_path, label in labeled_files:\n",
    "    audio, sr = sf.read(wav_path, dtype='float32')\n",
    "    if sample_rate is None:\n",
    "        sample_rate = sr\n",
    "    elif sr != sample_rate:\n",
    "        raise ValueError(f\"Sample rate mismatch for {wav_path}: {sr} vs {sample_rate}\")\n",
    "    dataset.append({'name': stem, 'audio': audio, 'label': label})\n",
    "    print(f\"Loaded {wav_path} label={label} samples={len(audio)} sr={sr}\")\n",
    "\n",
    "y = dataset[0]['audio']\n",
    "sr = sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92646f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "6701608f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0647583 ,  0.07305908,  0.07733154, ..., -0.00384521,\n",
       "        0.00158691,  0.00167847], shape=(1920000,), dtype=float32)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "04ecbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        if i + n > len(lst):\n",
    "            return\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "\n",
    "CHUNK_DURATION_S = 0.5  # 0.5s chunks\n",
    "SEGMENT_DURATION_S = 2.0  # 2s windows (4 chunks)\n",
    "SEGMENT_HOP_CHUNKS = 1    # slide by 1 chunk -> 0.5s hop\n",
    "\n",
    "\n",
    "def chunk_fft_features(signal, sample_rate, chunk_duration=CHUNK_DURATION_S):\n",
    "    \"\"\"Chunk audio and compute binned FFT magnitudes for each chunk.\"\"\"\n",
    "    chunk_size = int(sample_rate * chunk_duration)\n",
    "    chunked = np.asarray(list(chunks(signal, chunk_size)))\n",
    "    if chunked.size == 0:\n",
    "        return np.empty((0, 0), dtype=np.float32)\n",
    "\n",
    "    fft_mag = np.abs(np.fft.rfft(chunked, axis=1))\n",
    "    n_chunks, n_bins = fft_mag.shape\n",
    "\n",
    "    bin_width = sample_rate / chunk_size\n",
    "\n",
    "    def hz_to_bin(hz: float) -> int:\n",
    "        return int(np.floor(hz / bin_width))\n",
    "\n",
    "    bin_320 = min(hz_to_bin(320.0), n_bins)      # 0–320 Hz\n",
    "    bin_3200 = min(hz_to_bin(3200.0), n_bins)    # 320–3200 Hz\n",
    "\n",
    "    def merge_region(region: np.ndarray, target_bw_hz: float) -> np.ndarray:\n",
    "        if region.shape[1] == 0:\n",
    "            return region[:, :0]\n",
    "\n",
    "        bins_per = max(1, int(round(target_bw_hz / bin_width)))\n",
    "        usable = (region.shape[1] // bins_per) * bins_per\n",
    "        if usable == 0:\n",
    "            return region[:, :0]\n",
    "\n",
    "        region = region[:, :usable]\n",
    "        return region.reshape(n_chunks, -1, bins_per).mean(axis=2)\n",
    "\n",
    "    low_raw = fft_mag[:, :bin_320]            # ~0–320 Hz\n",
    "    mid_raw = fft_mag[:, bin_320:bin_3200]    # ~320–3200 Hz\n",
    "    high_raw = fft_mag[:, bin_3200:]          # >3200 Hz\n",
    "\n",
    "    low = merge_region(low_raw, 4.0)      # ≈ 4 Hz bins\n",
    "    mid = merge_region(mid_raw, 32.0)     # ≈ 32 Hz bins\n",
    "    high = merge_region(high_raw, 128.0)  # ≈ 128 Hz bins\n",
    "\n",
    "    features = np.concatenate([low, mid, high], axis=1)\n",
    "    return features.astype(np.float32)\n",
    "\n",
    "\n",
    "def make_segments(\n",
    "    feature_array,\n",
    "    segment_duration=SEGMENT_DURATION_S,\n",
    "    chunk_duration=CHUNK_DURATION_S,\n",
    "    hop_chunks=SEGMENT_HOP_CHUNKS,\n",
    "):\n",
    "    \"\"\"Group FFT chunks into overlapping segments (rolling windows).\"\"\"\n",
    "    chunks_per_segment = int(segment_duration / chunk_duration)\n",
    "    if chunks_per_segment <= 0:\n",
    "        raise ValueError(\"segments must include at least one chunk\")\n",
    "    hop_chunks = max(1, int(hop_chunks))\n",
    "\n",
    "    total_chunks = feature_array.shape[0]\n",
    "    if total_chunks < chunks_per_segment:\n",
    "        return np.empty((0, chunks_per_segment, feature_array.shape[1]), dtype=feature_array.dtype)\n",
    "\n",
    "    segments = []\n",
    "    for start in range(0, total_chunks - chunks_per_segment + 1, hop_chunks):\n",
    "        end = start + chunks_per_segment\n",
    "        segments.append(feature_array[start:end])\n",
    "\n",
    "    return np.stack(segments, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "e9f9dd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_room_2_9_dec_presence: segments=237, segment_shape=(4, 207)\n",
      "group_room_3_yazan_lab_9_dec_presence: segments=117, segment_shape=(4, 207)\n",
      "group_room_4_9_dec_presence: segments=237, segment_shape=(4, 207)\n",
      "group_room_5_9_dec_presence: segments=237, segment_shape=(4, 207)\n",
      "group_room_9_dec_presence: segments=117, segment_shape=(4, 207)\n",
      "test_presence: segments=7, segment_shape=(4, 207)\n",
      "training-lax_no_presence: segments=117, segment_shape=(4, 207)\n",
      "training-lax_presence: segments=117, segment_shape=(4, 207)\n",
      "training2-lax_no_presence: segments=117, segment_shape=(4, 207)\n",
      "training2-lax_presence: segments=117, segment_shape=(4, 207)\n",
      "data_final (1420, 4, 207) labels_final (1420,)\n"
     ]
    }
   ],
   "source": [
    "all_segments = []\n",
    "all_labels = []\n",
    "for entry in dataset:\n",
    "    feats = chunk_fft_features(entry['audio'], sr)\n",
    "    segs = make_segments(feats)\n",
    "    if len(segs) == 0:\n",
    "        continue\n",
    "    all_segments.append(segs)\n",
    "    all_labels.append(np.full(len(segs), entry['label'], dtype=np.float32))\n",
    "    print(f\"{entry['name']}: segments={len(segs)}, segment_shape={segs.shape[1:]}\")\n",
    "\n",
    "if not all_segments:\n",
    "    raise ValueError(\"No segments created from available wav files.\")\n",
    "\n",
    "data_final = np.concatenate(all_segments)\n",
    "labels_final = np.concatenate(all_labels)\n",
    "\n",
    "print(\"data_final\", data_final.shape, \"labels_final\", labels_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "62391056",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, train_labels, validation_labels = train_test_split(\n",
    "    data_final, labels_final, test_size=0.2, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "cc8ebf6d-369f-4ee4-8361-ac48805f6e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136, 4, 207)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e254cded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yazan.al-aswad/OtherCode/School/IOT/ML-for-iot-project-presence-detector/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m102\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_28 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m16\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m8\u001b[0m)       │           \u001b[38;5;34m520\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_29 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m8\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_23 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,081</span> (51.10 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,081\u001b[0m (51.10 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,081</span> (51.10 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,081\u001b[0m (51.10 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape data for Conv2D (time, freq, channel)\n",
    "train_data_reshaped = train_data[..., np.newaxis]\n",
    "validation_data_reshaped = validation_data[..., np.newaxis]\n",
    "\n",
    "input_shape = (train_data_reshaped.shape[1], train_data_reshaped.shape[2], 1)\n",
    "\n",
    "model = K.models.Sequential()\n",
    "model.add(K.layers.Conv2D(filters=16, kernel_size=(2, 4), strides=(1, 2), activation='relu', input_shape=input_shape))\n",
    "model.add(K.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 2)))\n",
    "model.add(K.layers.Conv2D(filters=8, kernel_size=(1, 4), strides=(1, 2), activation='relu'))\n",
    "model.add(K.layers.MaxPool2D(pool_size=(1, 2), strides=(1, 2)))\n",
    "model.add(K.layers.Flatten())\n",
    "model.add(K.layers.Dense(64, activation='relu'))\n",
    "model.add(K.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a8476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a9a454de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "checkpoint_cb = K.callbacks.ModelCheckpoint(\n",
    "    filepath=\"best.weights.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "7af0377b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m24/36\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8471 - loss: 0.2699   \n",
      "Epoch 1: val_accuracy improved from None to 0.95423, saving model to best.weights.h5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8829 - loss: 0.2166 - val_accuracy: 0.9542 - val_loss: 0.1662\n",
      "Epoch 2/10\n",
      "\u001b[1m24/36\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9503 - loss: 0.1603 \n",
      "Epoch 2: val_accuracy did not improve from 0.95423\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9569 - loss: 0.1383 - val_accuracy: 0.9542 - val_loss: 0.1136\n",
      "Epoch 3/10\n",
      "\u001b[1m25/36\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.1185 \n",
      "Epoch 3: val_accuracy did not improve from 0.95423\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9674 - loss: 0.1025 - val_accuracy: 0.9472 - val_loss: 0.1095\n",
      "Epoch 4/10\n",
      "\u001b[1m23/36\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0779 \n",
      "Epoch 4: val_accuracy improved from 0.95423 to 0.97183, saving model to best.weights.h5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.0872 - val_accuracy: 0.9718 - val_loss: 0.0673\n",
      "Epoch 5/10\n",
      "\u001b[1m23/36\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.0832 \n",
      "Epoch 5: val_accuracy improved from 0.97183 to 0.97535, saving model to best.weights.h5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.0834 - val_accuracy: 0.9754 - val_loss: 0.0619\n",
      "Epoch 6/10\n",
      "\u001b[1m23/36\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0485 \n",
      "Epoch 6: val_accuracy did not improve from 0.97535\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9806 - loss: 0.0563 - val_accuracy: 0.9754 - val_loss: 0.0673\n",
      "Epoch 7/10\n",
      "\u001b[1m23/36\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.0538 \n",
      "Epoch 7: val_accuracy improved from 0.97535 to 0.97887, saving model to best.weights.h5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9806 - loss: 0.0488 - val_accuracy: 0.9789 - val_loss: 0.0496\n",
      "Epoch 8/10\n",
      "\u001b[1m23/36\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9710 - loss: 0.0774 \n",
      "Epoch 8: val_accuracy did not improve from 0.97887\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9842 - loss: 0.0529 - val_accuracy: 0.9789 - val_loss: 0.0455\n",
      "Epoch 9/10\n",
      "\u001b[1m22/36\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9748 - loss: 0.0446 \n",
      "Epoch 9: val_accuracy did not improve from 0.97887\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9824 - loss: 0.0446 - val_accuracy: 0.9789 - val_loss: 0.0526\n",
      "Epoch 10/10\n",
      "\u001b[1m20/36\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9820 - loss: 0.0435 \n",
      "Epoch 10: val_accuracy improved from 0.97887 to 0.98239, saving model to best.weights.h5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9894 - loss: 0.0356 - val_accuracy: 0.9824 - val_loss: 0.0454\n",
      "Loaded best weights from best.weights.h5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data_reshaped,\n",
    "    train_labels,\n",
    "    validation_data=(validation_data_reshaped, validation_labels),\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint_cb],\n",
    ")\n",
    "\n",
    "best_path = \"best.weights.h5\"\n",
    "if Path(best_path).exists():\n",
    "    model.load_weights(best_path)\n",
    "    print(f\"Loaded best weights from {best_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "33134c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run model\n"
     ]
    }
   ],
   "source": [
    "print(\"run model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b290d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/nf/hck60h0n5dl_8jn9fz3mlz080000gp/T/tmp58u3p8p8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/nf/hck60h0n5dl_8jn9fz3mlz080000gp/T/tmp58u3p8p8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/nf/hck60h0n5dl_8jn9fz3mlz080000gp/T/tmp58u3p8p8'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 4, 207, 1), dtype=tf.float32, name='keras_tensor_598')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  14619716176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14619716368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14619715984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14619715600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14619713488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14619714256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14619714064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14619717712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1765292134.610879 10127952 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1765292134.610898 10127952 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-09 15:55:34.611022: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/nf/hck60h0n5dl_8jn9fz3mlz080000gp/T/tmp58u3p8p8\n",
      "2025-12-09 15:55:34.611493: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-12-09 15:55:34.611499: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/nf/hck60h0n5dl_8jn9fz3mlz080000gp/T/tmp58u3p8p8\n",
      "2025-12-09 15:55:34.615507: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-12-09 15:55:34.637979: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/nf/hck60h0n5dl_8jn9fz3mlz080000gp/T/tmp58u3p8p8\n",
      "2025-12-09 15:55:34.646016: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 34993 microseconds.\n"
     ]
    }
   ],
   "source": [
    "from tinymlgen import port\n",
    "\n",
    "c_code = port(model, variable_name='seizure_model', pretty_print=True,optimize=False)\n",
    "filename = 'arduino/net.h'\n",
    "with open(filename,'w') as f: \n",
    "    f.write(c_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
